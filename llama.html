<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Fine-Tuning LLaMa 3.2</title>
<link rel="stylesheet" href="https://unpkg.com/xp.css" />
<link rel = "stylesheet" href = "styles.css" /> 
</head>
<body>
<div class="window">
  <div class="title-bar">
    <div class="title-bar-text">Fine-Tuning LLaMa 3.2</div>
    <div class="title-bar-controls">
      <button aria-label="Minimize" onclick = "goBack();"></button>
      <button aria-label="Maximize"></button>
      <button aria-label="Close" onclick = "goBack();"></button>
    </div>
    <script> 

        function goBack() {
            window.location.href = 'index.html'
        }
    </script>
  </div>
  <div class="window-body">
    <menu role="tablist">
      <button aria-selected="true" aria-controls="about">About</button>
    </menu>

    <article role="tabpanel" id="about">
     <h1 style="font-size: 2.0rem; margin-bottom: 0.2rem;">Project Summary</h1>
  <div class="about-content">
    <p style="font-size: 16px;">
      This project explores fine-tuning pretrained open-source Large Language Models (LLMs) on two tasks where the base models struggle: 
      language translation (English-Spanish) and LeetCode-style Python coding problems. 
      Alongside this, I built and trained a character-level GPT transformer model from scratch to compare transfer learning against training from zero.


    </p>

    <p style="font-size: 16px;">
     I fine-tuned Meta’s LLaMA 3.2 1B model using efficient techniques like LoRA and 4-bit quantization to make the process accessible on limited hardware.
    <p style="font-size: 16px;">
      The fine-tuned translation model performed well, achieving a BLEU score around 0.4 (compared to ~0.15 for the base model), showing fine-tuning’s practical benefits even with limited resources. 
    </p>

    <p style="font-size: 16px;">
      This project gave me hands-on experience with Python’s <strong>Pytorch</strong> library,  <strong>Hugging Face</strong>  and <strong>LLM Fine-Tuning</strong>. 
      It also strengthened my understanding of modern machine learning workflows, from data preprocessing and model training to evaluation and optimization.
    </p>
  </div>
    </article>



    <section class="field-row" style="justify-content: flex-end">
      <a href="https://github.com/hadi-myi/Fine-Tuning-LLaMa-3.2">
      <button>Github</button>
       </a>
        </a>
    </section>
  </div>
</div>

<script>
const tabs = document.querySelectorAll('menu[role="tablist"] button');
const panels = document.querySelectorAll('article[role="tabpanel"]');

function setUniformArticleHeight() {
  
  const isMobile = window.matchMedia('(max-width: 768px)').matches;
  if (isMobile) {
   
    panels.forEach(a => a.style.minHeight = 'auto');
    return;
  }

  
  panels.forEach(a => a.style.minHeight = 'auto');

  let maxHeight = 0;
  panels.forEach(a => {
    const wasHidden = a.hidden;
    a.hidden = false; 
    maxHeight = Math.max(maxHeight, a.scrollHeight);
    a.hidden = wasHidden;
  });

  panels.forEach(a => a.style.minHeight = maxHeight + 'px');
}

tabs.forEach(tab => {
  tab.addEventListener('click', () => {
    tabs.forEach(btn => btn.setAttribute('aria-selected', 'false'));
    panels.forEach(panel => panel.hidden = true);
    tab.setAttribute('aria-selected', 'true');
    document.getElementById(tab.getAttribute('aria-controls')).hidden = false;
    
    setTimeout(setUniformArticleHeight, 50);
  });
});


window.addEventListener('load', setUniformArticleHeight);
window.addEventListener('resize', setUniformArticleHeight);

const mq = window.matchMedia('(max-width: 768px)');
mq.addEventListener?.('change', setUniformArticleHeight);
</script>
</body>
</html>